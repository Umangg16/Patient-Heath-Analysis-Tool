{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73170f3-a0a9-47fd-aa58-27bafc4b0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [openpyxl]\n",
      "\u001b[1A\u001b[2KSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a4037e8-40fc-41ff-b322-b91ef22bb249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyCVQ6BD_JjChBqrTuoYLle9Qh0IG9Cvu1g'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926014c1-3cdc-48c4-9c25-392841ae4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/umang1607/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/umang1607/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === SETUP ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data \n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15db7fcb-44b2-46f6-9497-72cc7dfd349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FILE PATHS ===\n",
    "ehr_path = \"/Users/umang1607/Desktop/Mimic_modeling/combined_diseases.xlsx\"\n",
    "sym2d_path = \"/Users/umang1607/Desktop/Mimic_modeling/Symptom2Disease.csv\"\n",
    "sym_train_path = \"/Users/umang1607/Desktop/Mimic_modeling/symptom-disease-train-dataset.csv\"\n",
    "mapping_path = \"/Users/umang1607/Desktop/Mimic_modeling/mapping.json\"\n",
    "synthetic_path = \"/Users/umang1607/Desktop/Mimic_modeling/synthetic_symptoms.csv\" \n",
    "\n",
    "# === TARGET DISEASES (normalize to canonical names) ===\n",
    "canon_map = {\n",
    "    \"psoriasis\": \"Psoriasis\",\n",
    "    \"pneumonia\": \"Pneumonia\",\n",
    "    \"hemorrhoid\": \"Dimorphic Hemorrhoids\",\n",
    "    \"arthritis\": \"Arthritis\",\n",
    "    \"asthma\": \"Bronchial Asthma\",\n",
    "    \"jaundice\": \"Jaundice\",\n",
    "    \"urinary tract\": \"Urinary Tract Infection\",\n",
    "    \"peptic ulcer\": \"Peptic Ulcer Disease\",\n",
    "    \"diabetes\": \"Diabetes\",\n",
    "    \"hypertension\": \"Hypertension\",\n",
    "    \"heart failure\": \"Heart Failure\",\n",
    "    \"kidney failure\": \"Kidney Failure\"\n",
    "}\n",
    "target_diseases = list(canon_map.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa40c165-d335-4237-92a5-261b13304e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1ï¸âƒ£ LOAD MAIN EHR FILE ===\n",
    "df_ehr = pd.read_excel(ehr_path)\n",
    "missing_cols = {'subject_id','gender','age','hadm_id','diagnosis_name','procedures','medications',\n",
    "                'lab_test','lab_value','valueuom','lab_time','note_text','note_time'} - set(df_ehr.columns)\n",
    "if missing_cols:\n",
    "    print(\"WARNING: EHR file is missing columns:\", missing_cols)\n",
    "\n",
    "# Standardize diagnosis_name\n",
    "def normalize_dx(name: str):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    s = str(name).strip().lower()\n",
    "    for k, v in canon_map.items():\n",
    "        if k in s:\n",
    "            return v\n",
    "    return str(name).strip()\n",
    "\n",
    "df_ehr['diagnosis_name'] = df_ehr['diagnosis_name'].apply(normalize_dx)\n",
    "df_ehr = df_ehr[df_ehr['diagnosis_name'].isin(target_diseases)].copy()\n",
    "df_ehr['note_text'] = df_ehr['note_text'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9936fb3-a90b-4d05-ac54-a07cf96e592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Synthetic + base symptom data merged successfully!\n",
      "disease\n",
      "Arthritis                  191\n",
      "Hypertension               190\n",
      "Pneumonia                  188\n",
      "Urinary Tract Infection    188\n",
      "Bronchial Asthma           186\n",
      "Diabetes                   183\n",
      "Peptic Ulcer Disease       180\n",
      "Psoriasis                  179\n",
      "Jaundice                   179\n",
      "Dimorphic Hemorrhoids       51\n",
      "Kidney Failure              26\n",
      "Heart Failure               26\n",
      "Name: count, dtype: int64\n",
      "sym2d columns: ['disease', 'text']\n"
     ]
    }
   ],
   "source": [
    "# === 2ï¸âƒ£ BUILD MASTER SYMPTOM TEXT TABLE (sym2d) ===\n",
    "# This will combine:\n",
    "#   - Symptom2Disease.csv\n",
    "#   - symptom-disease-train-dataset.csv (+ mapping.json)\n",
    "#   - synthetic_symptoms.csv  (Heart & Kidney Failure)\n",
    "# into a single DataFrame: sym2d[disease, text]\n",
    "\n",
    "def map_to_canon(name: str):\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    low = str(name).strip().lower()\n",
    "    for k, v in canon_map.items():\n",
    "        if k in low:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def comma_list_to_text(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    return \" \".join(\n",
    "        t.strip().replace(\"_\", \" \")\n",
    "        for t in str(s).split(\",\")\n",
    "        if t.strip()\n",
    "    )\n",
    "\n",
    "# ---------- A) From Symptom2Disease.csv ----------\n",
    "sym2d_raw = pd.read_csv(sym2d_path)\n",
    "\n",
    "# Assume columns: label, text\n",
    "sym2d_raw['label'] = sym2d_raw['label'].astype(str).str.strip()\n",
    "sym2d_raw['disease'] = sym2d_raw['label'].apply(map_to_canon)\n",
    "\n",
    "sym2d_a = sym2d_raw[sym2d_raw['disease'].isin(target_diseases)][['disease', 'text']].copy()\n",
    "\n",
    "# ---------- B) From symptom-disease-train-dataset.csv + mapping.json ----------\n",
    "with open(mapping_path, 'r') as f:\n",
    "    name_to_id = json.load(f)\n",
    "id_to_name = {v: k for k, v in name_to_id.items()}\n",
    "\n",
    "sym_train = pd.read_csv(sym_train_path)\n",
    "sym_train['disease_name'] = sym_train['label'].map(id_to_name).astype(str)\n",
    "sym_train['disease'] = sym_train['disease_name'].apply(map_to_canon)\n",
    "sym_train['text'] = sym_train['text'].apply(comma_list_to_text)\n",
    "\n",
    "sym2d_b = sym_train[sym_train['disease'].isin(target_diseases)][['disease', 'text']].copy()\n",
    "\n",
    "# ---------- C) From synthetic_symptoms.csv (Heart & Kidney Failure) ----------\n",
    "synthetic_sym = pd.read_csv(synthetic_path)\n",
    "synthetic_sym.columns = synthetic_sym.columns.str.strip().str.lower()\n",
    "synthetic_sym = synthetic_sym.rename(columns={'label': 'disease', 'symptom_text': 'text'})\n",
    "\n",
    "synthetic_sym['disease'] = synthetic_sym['disease'].astype(str).str.strip()\n",
    "synthetic_sym['text'] = synthetic_sym['text'].astype(str).str.strip()\n",
    "synthetic_sym['disease'] = synthetic_sym['disease'].apply(map_to_canon)\n",
    "synthetic_sym = synthetic_sym[synthetic_sym['disease'].isin(target_diseases)][['disease', 'text']].copy()\n",
    "\n",
    "# ---------- D) Combine all sources into ONE clean sym2d ----------\n",
    "sym2d = pd.concat([sym2d_a, sym2d_b, synthetic_sym], ignore_index=True)\n",
    "\n",
    "sym2d = sym2d.dropna(subset=['disease', 'text'])\n",
    "sym2d['disease'] = sym2d['disease'].astype(str).str.strip()\n",
    "sym2d['text'] = sym2d['text'].astype(str).str.strip()\n",
    "\n",
    "# Safety: ensure no duplicate column names\n",
    "sym2d = sym2d.loc[:, ~sym2d.columns.duplicated()]\n",
    "\n",
    "print(\"âœ… Synthetic + base symptom data merged successfully!\")\n",
    "print(sym2d['disease'].value_counts())\n",
    "print(\"sym2d columns:\", sym2d.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8997f10-5caa-41c1-93b8-76926a24a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sym2d columns: ['disease', 'text']\n",
      "synthetic_sym columns: ['disease', 'text']\n",
      "synthetic_sym duplicates: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"sym2d columns:\", sym2d.columns.tolist())\n",
    "print(\"synthetic_sym columns:\", synthetic_sym.columns.tolist())\n",
    "print(\"synthetic_sym duplicates:\", synthetic_sym.columns[synthetic_sym.columns.duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a61e648-6096-42e4-b0b8-c45f15bdc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Randomized symptom text assigned per patient.\n",
      "  diagnosis_name                                       symptom_text\n",
      "0      Psoriasis  skin rash joint pain skin peeling small dents ...\n",
      "1      Psoriasis  I have been having rashes on my skin. The rash...\n",
      "2      Psoriasis  skin rash joint pain skin peeling silver like ...\n",
      "3      Psoriasis  skin rash joint pain skin peeling small dents ...\n",
      "4      Psoriasis  The rash on my skin has spread to other parts ...\n",
      "5      Psoriasis  My nails have small dents on them. Even my joi...\n",
      "6      Psoriasis  I have noticed that my skin has become more se...\n",
      "7      Psoriasis  skin rash joint pain skin peeling silver like ...\n",
      "8      Psoriasis  My skin is breaking out in rashes. When I scra...\n",
      "9      Psoriasis  skin rash joint pain silver like dusting small...\n"
     ]
    }
   ],
   "source": [
    "# === 5ï¸âƒ£ BUILD PER-DISEASE SYMPTOM CORPUS ===\n",
    "corpus_from_sym2d = sym2d.groupby('disease')['text'].apply(lambda x: \" \".join(map(str, x))).to_dict()\n",
    "corpus_from_symtrain = sym_train.groupby('disease')['text'].apply(lambda x: \" \".join(map(str, x))).to_dict()\n",
    "\n",
    "disease_corpus = {d: \"\" for d in target_diseases}\n",
    "for d in target_diseases:\n",
    "    d_texts = []\n",
    "    if d in corpus_from_sym2d:\n",
    "        d_texts.append(corpus_from_sym2d[d])\n",
    "    if d in corpus_from_symtrain:\n",
    "        d_texts.append(corpus_from_symtrain[d])\n",
    "    disease_corpus[d] = \" \".join(d_texts).strip()\n",
    "\n",
    "# Assign random symptom text per patient\n",
    "disease_sym_map = sym2d.groupby(\"disease\")[\"text\"].apply(list).to_dict()\n",
    "\n",
    "def random_symptom(disease):\n",
    "    disease = str(disease).strip()\n",
    "    if disease in disease_sym_map:\n",
    "        return np.random.choice(disease_sym_map[disease], replace=True)\n",
    "    return \"\"\n",
    "\n",
    "df_ehr[\"symptom_text\"] = df_ehr[\"diagnosis_name\"].apply(random_symptom)\n",
    "df_ehr[\"text_all\"] = (df_ehr[\"note_text\"].astype(str) + \" \" + df_ehr[\"symptom_text\"].astype(str)).str.strip()\n",
    "\n",
    "print(\"âœ… Randomized symptom text assigned per patient.\")\n",
    "print(df_ehr[[\"diagnosis_name\", \"symptom_text\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbe8725-f8a6-4bd5-a968-a51c9d5684ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     diagnosis_name                                       symptom_text\n",
      "0         Psoriasis  skin rash joint pain skin peeling small dents ...\n",
      "1         Psoriasis  I have been having rashes on my skin. The rash...\n",
      "2         Psoriasis  skin rash joint pain skin peeling silver like ...\n",
      "3         Psoriasis  skin rash joint pain skin peeling small dents ...\n",
      "4         Psoriasis  The rash on my skin has spread to other parts ...\n",
      "...             ...                                                ...\n",
      "1495      Psoriasis  I have been having pain in my joints and I hav...\n",
      "1496      Psoriasis  I have a rash on my skin that is red itchy and...\n",
      "1497      Psoriasis  skin rash joint pain skin peeling silver like ...\n",
      "1498      Psoriasis  For the past few weeks, I've had a skin rash o...\n",
      "1499      Psoriasis  skin rash joint pain skin peeling small dents ...\n",
      "\n",
      "[1500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_ehr[df_ehr[\"diagnosis_name\"] == \"Psoriasis\"][[\"diagnosis_name\", \"symptom_text\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178927fc-6cd4-4602-bfe8-8cbd353d54e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved merged file -> combined_with_symptoms.csv   rows=18000\n"
     ]
    }
   ],
   "source": [
    "# Save merged file\n",
    "out_csv = \"combined_with_symptoms.csv\"\n",
    "df_ehr.to_csv(out_csv, index=False)\n",
    "print(f\"âœ… Saved merged file -> {out_csv}   rows={len(df_ehr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467c673e-4eea-48ac-8a8d-29f6f290f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5) BASIC STRUCTURED FEATURES (age, gender; plus simple lab aggregation) ===\n",
    "# Gender standardize\n",
    "df_ehr['gender'] = df_ehr['gender'].astype(str).str.upper().replace({'MALE':'M','FEMALE':'F'})\n",
    "\n",
    "# Aggregate labs per (subject_id, hadm_id)\n",
    "lab_agg = (df_ehr.groupby(['subject_id','hadm_id','lab_test'])['lab_value']\n",
    "           .agg(['mean','min','max']).reset_index())\n",
    "lab_pivot = lab_agg.pivot_table(index=['subject_id','hadm_id'],\n",
    "                                columns='lab_test',\n",
    "                                values=['mean','min','max'])\n",
    "lab_pivot.columns = ['_'.join(col).strip() for col in lab_pivot.columns.values]\n",
    "lab_pivot = lab_pivot.reset_index()\n",
    "\n",
    "# Deduplicate EHR core\n",
    "core = df_ehr[['subject_id','hadm_id','age','gender','diagnosis_name','procedures','medications','text_all']].drop_duplicates(\n",
    "    subset=['subject_id','hadm_id']\n",
    ")\n",
    "X = core.merge(lab_pivot, on=['subject_id','hadm_id'], how='left')\n",
    "\n",
    "# Target\n",
    "y = X['diagnosis_name'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43255026-8885-4b3c-8d69-35addacd2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6) TEXT CLEANING (lowercase, keep letters, lemmatize, stopword filter) ===\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'[^a-z\\s]', ' ', s)\n",
    "    tokens = [w for w in s.split() if w not in stop and len(w) > 2]\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "X['text_clean'] = X['text_all'].fillna(\"\").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e47355-511e-4608-be2f-671b1e32f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7) FEATURE SPLIT ===\n",
    "text_col = 'text_clean'\n",
    "cat_cols  = ['gender']        \n",
    "num_cols  = ['age'] + [c for c in X.columns if c.startswith(('mean_','min_','max_'))]\n",
    "\n",
    "# Impute numeric (median)\n",
    "X[num_cols] = X[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
    "\n",
    "# One-hot categorical\n",
    "X[cat_cols] = X[cat_cols].fillna('Unknown')\n",
    "\n",
    "# Train/test split (stratified)\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X[[text_col] + cat_cols + num_cols],\n",
    "    y_enc,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5ca9d4-780e-44ef-804c-69884a9a205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8) VECTORIZERS / TRANSFORMERS ===\n",
    "# Text: TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "\n",
    "# Categorical+Numeric preprocessing via ColumnTransformer\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_mean=False))  # with_mean=False for sparse safety\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Fit text on train\n",
    "X_train_text = tfidf.fit_transform(X_train[text_col])\n",
    "X_test_text  = tfidf.transform(X_test[text_col])\n",
    "\n",
    "# Build structured matrix\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cats', cat_transformer, cat_cols),\n",
    "        ('nums', num_transformer, num_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "X_train_struct = ct.fit_transform(X_train)\n",
    "X_test_struct  = ct.transform(X_test)\n",
    "\n",
    "# Combine text + structured\n",
    "from scipy.sparse import hstack\n",
    "Xtr = hstack([X_train_text, X_train_struct])  # train features\n",
    "Xte = hstack([X_test_text,  X_test_struct])   # test  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51796621-2b46-4c57-92ba-c205857695ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest       ->  F1 (macro): 0.9441 | AUROC (macro): 0.9959\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=24,\n",
    "    min_samples_leaf=3,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(Xtr, y_train)\n",
    "y_pred_rf = rf.predict(Xte)\n",
    "y_prob_rf = rf.predict_proba(Xte)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
    "auroc_rf = roc_auc_score(y_test, y_prob_rf, multi_class='ovr', average='macro')\n",
    "\n",
    "print(f\"Random Forest       ->  F1 (macro): {f1_rf:.4f} | AUROC (macro): {auroc_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f28a1f3-e103-443b-97da-6987d7252a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully as disease_rf_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the trained Random Forest model\n",
    "joblib.dump(rf, \"models/disease_rf_model.pkl\")\n",
    "print(\"Model saved successfully as disease_rf_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f4ee091-3027-4584-aa86-ca29d205b648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF vectorizer and ColumnTransformer saved.\n",
      "âœ… Metadata saved as models/disease_rf_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# 2) Save the transformers used to build Xtr / Xte\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(tfidf, \"models/tfidf_vectorizer.pkl\")\n",
    "joblib.dump(ct, \"models/column_transformer.pkl\")\n",
    "print(\"âœ… TF-IDF vectorizer and ColumnTransformer saved.\")\n",
    "\n",
    "# 3) Build feature names properly (no .columns on sparse matrices!)\n",
    "text_feature_names = tfidf.get_feature_names_out().tolist()          # from TF-IDF\n",
    "struct_feature_names = ct.get_feature_names_out().tolist()           # from ColumnTransformer\n",
    "\n",
    "combined_feature_names = text_feature_names + struct_feature_names   # order matches hstack\n",
    "\n",
    "# 4) Save metadata\n",
    "metadata = {\n",
    "    \"text_col\": text_col,               # your text column name\n",
    "    \"cat_cols\": list(cat_cols),         # categorical columns used in ct\n",
    "    \"num_cols\": list(num_cols),         # numeric columns used in ct\n",
    "    \"feature_names\": combined_feature_names,\n",
    "    \"target_classes\": np.unique(y_train).tolist()\n",
    "}\n",
    "\n",
    "with open(\"models/disease_rf_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"âœ… Metadata saved as models/disease_rf_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b482bf-ef30-469b-9f4f-aa35bd98d420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/label_encoder.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(le, \"models/label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "823839e4-cffb-4340-adc4-0e822d2f1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/model_service.py (or a Jupyter cell)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "BASE_DIR = Path(\".\")\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "class DiseasePredictor:\n",
    "    def __init__(self, model_dir: Path = MODEL_DIR):\n",
    "        self.model_dir = Path(model_dir)\n",
    "\n",
    "        # Load model + transformers\n",
    "        self.rf = joblib.load(self.model_dir / \"disease_rf_model.pkl\")\n",
    "        self.tfidf = joblib.load(self.model_dir / \"tfidf_vectorizer.pkl\")\n",
    "        self.ct = joblib.load(self.model_dir / \"column_transformer.pkl\")\n",
    "        self.label_encoder = joblib.load(self.model_dir / \"label_encoder.pkl\")\n",
    "\n",
    "        # Load metadata\n",
    "        with open(self.model_dir / \"disease_rf_metadata.json\") as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "        self.text_col = meta[\"text_col\"]\n",
    "        self.cat_cols = meta[\"cat_cols\"]\n",
    "        self.num_cols = meta[\"num_cols\"]\n",
    "        self.feature_names = meta[\"feature_names\"]\n",
    "        self.target_classes = meta[\"target_classes\"]  # these are encoded ints\n",
    "\n",
    "        # Optional: store original disease label names for convenience\n",
    "        self.class_labels = list(self.label_encoder.classes_)\n",
    "\n",
    "\n",
    "    def _build_design_matrix_from_df(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Given a DataFrame with at least text_col, cat_cols, num_cols,\n",
    "        return the sparse matrix X ready for RF.predict_proba.\n",
    "        \"\"\"\n",
    "        # Text part\n",
    "        X_text = self.tfidf.transform(df[self.text_col].fillna(\"\"))\n",
    "\n",
    "        # Structured part\n",
    "        X_struct = self.ct.transform(df[self.cat_cols + self.num_cols])\n",
    "\n",
    "        # Combine in the same order as during training\n",
    "        X = hstack([X_text, X_struct])\n",
    "        return X\n",
    "\n",
    "    def predict_proba_from_df(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        X = self._build_design_matrix_from_df(df)\n",
    "        return self.rf.predict_proba(X)\n",
    "\n",
    "    def predict_topk_from_df(self, df: pd.DataFrame, k: int = 3, threshold: float = 0.1):\n",
    "        proba = self.predict_proba_from_df(df)[0]  # probabilities in same order as rf.classes_\n",
    "        probs_dict = dict(zip(self.rf.classes_, proba))  # key: encoded int, val: prob\n",
    "\n",
    "        # Sort by probability (descending)\n",
    "        sorted_items = sorted(probs_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Decode to original labels\n",
    "        decoded_results = []\n",
    "        for enc_label, p in sorted_items:\n",
    "            if p < threshold:\n",
    "                continue\n",
    "            # enc_label is an int: decode using label encoder\n",
    "            disease_name = self.label_encoder.inverse_transform([enc_label])[0]\n",
    "            decoded_results.append((disease_name, float(p)))\n",
    "\n",
    "        return decoded_results[:k]\n",
    "\n",
    "    def predict_from_inputs(self, text: str, cats: dict, nums: dict, k: int = 3, threshold: float = 0.1):\n",
    "        \"\"\"\n",
    "        Convenience method: build a 1-row DataFrame from raw inputs.\n",
    "\n",
    "        text: free-text symptom description for text_col\n",
    "        cats: dict for categorical columns (e.g. {'sex': 'M'})\n",
    "        nums: dict for numeric columns (e.g. {'age': 55, 'bmi': 30.2})\n",
    "        \"\"\"\n",
    "        row = {}\n",
    "\n",
    "        # text col\n",
    "        row[self.text_col] = text\n",
    "\n",
    "        # categorical\n",
    "        for col in self.cat_cols:\n",
    "            row[col] = cats.get(col, None)\n",
    "\n",
    "        # numeric\n",
    "        for col in self.num_cols:\n",
    "            row[col] = nums.get(col, np.nan)\n",
    "\n",
    "        df = pd.DataFrame([row])\n",
    "        return self.predict_topk_from_df(df, k=k, threshold=threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7399bd5a-1e7c-4665-952a-c77f5541c3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Psoriasis', 0.20525733154718234),\n",
       " ('Heart Failure', 0.17981375079647346),\n",
       " ('Kidney Failure', 0.1631428412671951)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from src.model_service import DiseasePredictor\n",
    "\n",
    "predictor = DiseasePredictor()\n",
    "\n",
    "text = \"I have a rash on my arms and neck that bleeds and My joints are experiencing an unusual discomfort\"\n",
    "cats = {\"sex\": \"M\"}              # adapt to your cat_cols\n",
    "nums = {\"age\": 55, \"bmi\": 30}  # adapt to your num_cols\n",
    "\n",
    "predictor.predict_from_inputs(text, cats, nums, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "541cbd48-ac06-462e-b8e4-d47311bf58f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NHANES stats + guidelines saved to data/nhanes_guidelines_2021_2023.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "nhanes = pd.read_csv(\"nhanes_2021_2023_integrated.csv\")  # adjust path if needed\n",
    "\n",
    "def derive_disease_category(row):\n",
    "    # ðŸ‘‰ IMPORTANT: use names that match your RF label encoder if possible\n",
    "    if row[\"has_cvd\"] == 1:\n",
    "        return \"cardiovascular_disease\"\n",
    "    elif row[\"has_heart_failure\"] == 1 or row[\"has_heart_attack\"] == 1 or row[\"has_coronary_disease\"] == 1 or row[\"has_angina\"] == 1:\n",
    "        return \"cardiovascular_disease\"\n",
    "    elif row[\"has_diabetes\"] == 1:\n",
    "        return \"diabetes\"\n",
    "    elif row[\"has_hypertension\"] == 1:\n",
    "        return \"hypertension\"\n",
    "    elif row[\"has_stroke\"] == 1:\n",
    "        return \"stroke\"\n",
    "    elif row[\"has_copd\"] == 1:\n",
    "        return \"copd\"\n",
    "    elif row[\"has_liver_condition\"] == 1:\n",
    "        return \"liver_disease\"\n",
    "    elif row[\"has_thyroid_problem\"] == 1:\n",
    "        return \"thyroid_disorder\"\n",
    "    else:\n",
    "        return \"no_major_condition\"\n",
    "\n",
    "nhanes[\"disease_category\"] = nhanes.apply(derive_disease_category, axis=1)\n",
    "\n",
    "# ---- Age bands ----\n",
    "bins = [0, 17, 29, 44, 59, 120]\n",
    "labels = [\"0-17\", \"18-29\", \"30-44\", \"45-59\", \"60+\"]\n",
    "nhanes[\"age_band\"] = pd.cut(nhanes[\"age\"], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# ---- Aggregate stats per disease + age band ----\n",
    "agg = (\n",
    "    nhanes\n",
    "    .groupby([\"disease_category\", \"age_band\"], observed=False)\n",
    "    .agg(\n",
    "        sugar_median=(\"sugar_g\", \"median\"),\n",
    "        sugar_mean=(\"sugar_g\", \"mean\"),\n",
    "        sodium_median=(\"sodium_mg\", \"median\"),\n",
    "        sodium_mean=(\"sodium_mg\", \"mean\"),\n",
    "        bmi_median=(\"bmi\", \"median\"),\n",
    "        bmi_mean=(\"bmi\", \"mean\"),\n",
    "        activity_median=(\"weekly_activity_mins\", \"median\"),\n",
    "        activity_mean=(\"weekly_activity_mins\", \"mean\"),\n",
    "        fiber_median=(\"fiber_g\", \"median\"),\n",
    "        fiber_mean=(\"fiber_g\", \"mean\"),\n",
    "        protein_median=(\"protein_g\", \"median\"),\n",
    "        protein_mean=(\"protein_g\", \"mean\"),\n",
    "        alcohol_median=(\"alcohol_g\", \"median\"),\n",
    "        alcohol_mean=(\"alcohol_g\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ---- Guideline thresholds ----\n",
    "guidelines = {\n",
    "    \"sugar\": {\"aha_limit_g\": 25, \"who_limit_g\": 50},\n",
    "    \"sodium\": {\"limit_mg\": 2300},\n",
    "    \"activity\": {\"min_minutes_week\": 150},\n",
    "    \"bmi\": {\"healthy_range\": [18.5, 24.9]},\n",
    "    \"fiber\": {\"min_g\": 25},\n",
    "    \"protein\": {\"min_g\": 50, \"max_g\": 60},  # general guideline if weight unknown\n",
    "    \"alcohol\": {\"max_g\": 14}  # conservative guideline\n",
    "}\n",
    "\n",
    "\n",
    "out = {\n",
    "    \"nhanes_stats\": agg.to_dict(orient=\"records\"),\n",
    "    \"guidelines\": guidelines\n",
    "}\n",
    "\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "with open(\"data/nhanes_guidelines_2021_2023.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=2)\n",
    "\n",
    "print(\"âœ… NHANES stats + guidelines saved to data/nhanes_guidelines_2021_2023.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
